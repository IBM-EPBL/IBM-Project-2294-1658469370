# -*- coding: utf-8 -*-
"""Lekhasri.K_Ass3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VZ9IRbLrXZVxZz4IPDWpSqiCop3OuHkZ
"""

import pandas as pd
import numpy as np
import seaborn as sns

"""LOAD THE DATASET"""

df=pd.read_csv('/content/abalone.csv')
df

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

"""Perform Below Visualizations.

1)Univariate Analysis
"""

plt.scatter(df.index,df['Length'])
plt.show();

sns.scatterplot(x=df.index,y=df['Length'], hue=df['Sex'])

sns.set(rc={'figure.figsize': (5,5)})
sns.set (font_scale=1.3)
fig=sns.lineplot (x=df.index, y=df['Diameter'], markevery=1, marker='d', data=df, hue=df ['Sex'])
fig.set(xlabel='index')

sns.stripplot (y=df['Length'])

plt.hist(df['Diameter'])

plt.figure (figsize=(7,7))
df ['Length'].plot (kind='density')

plt.boxplot(df['Diameter'])

"""2) Bi-Variate Analysis"""

sns.barplot(x='Sex',y='Rings',data=df)
sns.countplot(x='Sex',data=df)

sns.violinplot (x="Sex",y="Height",data=df, size=5)
plt.show()

sns.boxplot (x='Sex' ,y='Diameter' , data=df)
plt.show()

"""3)Multi-Variate Analysis

"""

sns.pairplot (df, hue="Sex",size=2)
plt.show()

"""4)Perform descriptive statistics on the dataset."""

pd.set_option('display.width', 150)
pd.set_option('precision', 3)
desc = df.describe()
print(desc)

"""5)Check for Missing values and deal with them"""

df.isnull()

df.notnull()

sns.heatmap(df.isnull(),yticklabels=False,cbar=False)

"""6)Find the outliers and replace them outliers"""

sns.boxplot(df['Height'],data=df)

df['Rings'].hist()

sns.boxplot(x=df['Rings'])

sns.boxplot(df['Rings'],data=df)

fare_mean = df['Rings'].mean()
fare_std = df['Rings'].std()
low = fare_mean -(3 * fare_std)
high = fare_mean +(3 * fare_std)
fare_outliers = df[(df['Rings']<low) | (df['Rings']>high)]
fare_outliers.head()

""" 7)Check for Categorical columns and perform encoding"""

from sklearn.compose import make_column_selector as selector
categorical_columns_selector = selector(dtype_include=object)
categorical_columns = categorical_columns_selector(df)
categorical_columns

data = df[categorical_columns]
data.head()

from sklearn.preprocessing import OrdinalEncoder
Sex_column = data[["Sex"]]
encoder = OrdinalEncoder()
Sex_encoded = encoder.fit_transform(Sex_column)
Sex_encoded

encoder.categories_

data_encoded = encoder.fit_transform(data)
data_encoded[:5]

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=False)
Sex_encoded = encoder.fit_transform(Sex_column)
Sex_encoded

feature_names = encoder.get_feature_names_out(input_features=["Sex"])
Sex_encoded = pd.DataFrame(Sex_encoded, columns=feature_names)
Sex_encoded.head()

data_encoded = encoder.fit_transform(data)
data_encoded[:5]

"""8. Split the data into dependent and independent variables"""

X= df.iloc[ : , :-1].values
print(X)

Y= df.iloc[: , 4].values
print(Y)

"""9)SCALE THE INDEPENDENT VARIABLES"""

from sklearn import preprocessing
df.drop(labels="Sex",axis=1)

"""10)SPLIT THE DATA INTO TRAINING AND TESTING"""

from sklearn.model_selection import train_test_split
X=df.iloc[ : , :-1]
y=df.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)
X_train

y_train

"""11,12&13) BUILD,TRAIN & TEST THE MODEL"""

train, test = train_test_split(df, test_size=0.30, random_state=1)
print('Train data points :', len(train))
print('Test data points :', len(test))

train.Sex = train.Sex.replace({"M":1, "I":0, "F":2})
test.Sex = test.Sex.replace({"M":1, "I":0, "F":2})

numerical_features = ["Length", 'Diameter', 'Height','Whole weight',
                      'Shucked weight', 'Viscera weight', 'Shell weight']
categorical_feature = "Sex"
features = numerical_features + [categorical_feature]
target = 'Rings'

fig, axes = plt.subplots(ncols=2,figsize=(20,8))
train[target].plot.hist(color='red', ax=axes[0])
axes[0].set(title="Train")
test[target].plot.hist(color='blue', ax=axes[1])
axes[1].set(title="Test")
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(4,2,figsize=(10, 14))
axes = np.ravel(axes)
for i, c in enumerate(numerical_features):
    hist = train[c].plot(kind = 'hist', ax=axes[i], title=c, color='yellow', bins=20)
plt.tight_layout()
plt.show()

idx = train.loc[train.Height>0.4].index
train.drop(idx, inplace=True)
idx = train.loc[train['Viscera weight']>0.6].index
train.drop(idx, inplace=True)
idx = train.loc[train[target]>25].index
train.drop(idx, inplace=True)
X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]
X_train.head()

"""14)MEASURE THE PERFORMANCE USING METRICS"""

from sklearn.linear_model import LinearRegression
from sklearn. linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
models = {'linear_regression':LinearRegression(),
          'lasso':Lasso(random_state=1),
          'decision_tree':DecisionTreeRegressor(random_state=1),
         'random_forest':RandomForestRegressor(random_state=1),
          'xgboost':XGBRegressor(random_state=1),
  }

# XGBoost
xgb_params = {'n_estimators':[100, 200, 300] , 
             'max_depth':list(range(1,10)) , 
             'learning_rate':[0.006,0.007,0.008,0.05,0.09] ,
             'min_child_weight':list(range(1,10))}

# Random Forest
rf_params = {'bootstrap': [True, False],
             'max_depth': [2, 5, 10, 20, None],
             'max_features': ['auto', 'sqrt'],
             'min_samples_leaf': [1, 2, 4],
             'min_samples_split': [2, 5, 10],
             'n_estimators': [100, 150, 200, 250]}

# Decision tree
dt_params =  {'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],
            'min_samples_split': [5, 10, 20, 30, 40, 50],
            'max_features': [0.2, 0.4, 0.6, 0.8, 1],
            'max_leaf_nodes': [8, 16, 32, 64, 128,256]}

# Lasso
lasso_params = {'alpha': [1e-4, 1e-3, 1e-2, 1, 10, 100]}

# Linear regression
lr_params = {'fit_intercept':[True,False]}

from sklearn.model_selection import RandomizedSearchCV
params = [lr_params, lasso_params, dt_params, rf_params, xgb_params]
# searching Hyperparameters
i=0
for name, model in models.items():
    print(name)
    regressor = RandomizedSearchCV(estimator = model,
    n_iter=10,
    param_distributions = params[i],
    cv = 3,
    scoring = 'neg_root_mean_squared_error')
    search = regressor.fit(X_train, y_train)
    print('Best params :',search.best_params_)
    print("RMSE :", -search.best_score_)
    i+=1
    print()

rf_params = {'n_estimators': 200, 
   'min_samples_split': 2,
    'min_samples_leaf': 4, 
    'max_features': 'sqrt', 
    'max_depth': None, 
    'bootstrap': True}
model = RandomForestRegressor(random_state=1, **rf_params)
model.fit(X_train, y_train)

import pickle 
with open("model.pkl", "wb") as f:pickle.dump(model, f)

y_pred = model.predict(X_test)
fig = plt.figure(figsize=(10, 6))
plt.scatter(range(y_test.shape[0]), y_test, color='brown', label='y_true')
plt.scatter(range(y_test.shape[0]), y_pred, color='pink', label='y_pred')
plt.legend()
plt.show()

def predict_age(x):
    x = pd.DataFrame([x], columns=features)
    age = model.predict(x)
    return round(age[0],2)
with open("model.pkl", 'rb') as f:
    model = pickle.load(f)
fx = [0.3 , 0.25 , 0.8  , 0.4 , 0.04, 0.02 , 0.04  , 0.1]
print("Estimated age : ",predict_age(fx))